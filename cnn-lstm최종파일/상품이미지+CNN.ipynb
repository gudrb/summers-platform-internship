{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pymssql\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from pandas import Series,DataFrame\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,Convolution2D,Activation,MaxPooling2D,Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shoda4000.pickle','rb') as fr: #피클 파일 불러와서 합치기\n",
    "    result3 = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3836\n",
       "2    3560\n",
       "6    3114\n",
       "1    3076\n",
       "4    2991\n",
       "3    2518\n",
       "7    2455\n",
       "5    2396\n",
       "Name: LABEL, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3.LABEL.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#다른 크기 이미지 제거후 인덱스 섞어주기\n",
    "for index in result3.index:\n",
    "    if result3.loc[index]['pl_imgurl'].shape != (28,28,3): \n",
    "        result3.drop(index, inplace = True)\n",
    "result3 = pd.concat([result3],ignore_index=True) \n",
    "result3 = result3.reindex(np.random.permutation(result3.index)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#원핫벡터 정답 label 만들기\n",
    "from keras.utils.np_utils import to_categorical\n",
    "labels = to_categorical(result3['LABEL'], num_classes =8) \n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#트레이닝 테스트 셋 분할\n",
    "data_train_image,data_test_image,labels_train_image,labels_test_image = train_test_split(np.array(result3['pl_imgurl'].tolist()),labels, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 3..., padding=\"same\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14341 samples, validate on 3586 samples\n",
      "Epoch 1/100\n",
      "14341/14341 [==============================] - ETA: 1:56 - loss: 2.1052 - acc: 0.120 - ETA: 1:02 - loss: 2.0960 - acc: 0.115 - ETA: 43s - loss: 2.0835 - acc: 0.131 - ETA: 34s - loss: 2.0783 - acc: 0.14 - ETA: 28s - loss: 2.0762 - acc: 0.13 - ETA: 25s - loss: 2.0754 - acc: 0.13 - ETA: 22s - loss: 2.0793 - acc: 0.13 - ETA: 20s - loss: 2.0778 - acc: 0.13 - ETA: 18s - loss: 2.0785 - acc: 0.13 - ETA: 17s - loss: 2.0752 - acc: 0.14 - ETA: 16s - loss: 2.0733 - acc: 0.14 - ETA: 15s - loss: 2.0717 - acc: 0.14 - ETA: 14s - loss: 2.0707 - acc: 0.14 - ETA: 13s - loss: 2.0717 - acc: 0.14 - ETA: 13s - loss: 2.0708 - acc: 0.14 - ETA: 12s - loss: 2.0703 - acc: 0.14 - ETA: 12s - loss: 2.0698 - acc: 0.14 - ETA: 11s - loss: 2.0692 - acc: 0.14 - ETA: 11s - loss: 2.0682 - acc: 0.14 - ETA: 10s - loss: 2.0670 - acc: 0.15 - ETA: 10s - loss: 2.0661 - acc: 0.15 - ETA: 10s - loss: 2.0649 - acc: 0.15 - ETA: 9s - loss: 2.0647 - acc: 0.1515 - ETA: 9s - loss: 2.0621 - acc: 0.153 - ETA: 9s - loss: 2.0611 - acc: 0.153 - ETA: 8s - loss: 2.0601 - acc: 0.153 - ETA: 8s - loss: 2.0598 - acc: 0.155 - ETA: 8s - loss: 2.0592 - acc: 0.154 - ETA: 7s - loss: 2.0578 - acc: 0.157 - ETA: 7s - loss: 2.0563 - acc: 0.160 - ETA: 7s - loss: 2.0555 - acc: 0.160 - ETA: 7s - loss: 2.0549 - acc: 0.161 - ETA: 6s - loss: 2.0532 - acc: 0.163 - ETA: 6s - loss: 2.0522 - acc: 0.163 - ETA: 6s - loss: 2.0514 - acc: 0.164 - ETA: 6s - loss: 2.0495 - acc: 0.165 - ETA: 6s - loss: 2.0485 - acc: 0.167 - ETA: 5s - loss: 2.0480 - acc: 0.167 - ETA: 5s - loss: 2.0466 - acc: 0.169 - ETA: 5s - loss: 2.0440 - acc: 0.171 - ETA: 5s - loss: 2.0419 - acc: 0.172 - ETA: 4s - loss: 2.0405 - acc: 0.172 - ETA: 4s - loss: 2.0375 - acc: 0.174 - ETA: 4s - loss: 2.0365 - acc: 0.175 - ETA: 4s - loss: 2.0352 - acc: 0.176 - ETA: 4s - loss: 2.0347 - acc: 0.177 - ETA: 4s - loss: 2.0321 - acc: 0.178 - ETA: 3s - loss: 2.0304 - acc: 0.179 - ETA: 3s - loss: 2.0292 - acc: 0.180 - ETA: 3s - loss: 2.0279 - acc: 0.181 - ETA: 3s - loss: 2.0267 - acc: 0.183 - ETA: 3s - loss: 2.0253 - acc: 0.184 - ETA: 3s - loss: 2.0232 - acc: 0.185 - ETA: 2s - loss: 2.0218 - acc: 0.187 - ETA: 2s - loss: 2.0202 - acc: 0.188 - ETA: 2s - loss: 2.0183 - acc: 0.189 - ETA: 2s - loss: 2.0174 - acc: 0.189 - ETA: 2s - loss: 2.0166 - acc: 0.190 - ETA: 2s - loss: 2.0141 - acc: 0.191 - ETA: 1s - loss: 2.0139 - acc: 0.191 - ETA: 1s - loss: 2.0129 - acc: 0.192 - ETA: 1s - loss: 2.0112 - acc: 0.192 - ETA: 1s - loss: 2.0102 - acc: 0.193 - ETA: 1s - loss: 2.0090 - acc: 0.195 - ETA: 1s - loss: 2.0071 - acc: 0.195 - ETA: 0s - loss: 2.0053 - acc: 0.197 - ETA: 0s - loss: 2.0039 - acc: 0.198 - ETA: 0s - loss: 2.0035 - acc: 0.199 - ETA: 0s - loss: 2.0020 - acc: 0.200 - ETA: 0s - loss: 2.0006 - acc: 0.201 - ETA: 0s - loss: 1.9990 - acc: 0.202 - 12s 823us/step - loss: 1.9982 - acc: 0.2028 - val_loss: 1.9058 - val_acc: 0.2680\n",
      "Epoch 2/100\n",
      "14341/14341 [==============================] - ETA: 9s - loss: 1.9101 - acc: 0.255 - ETA: 8s - loss: 1.8996 - acc: 0.252 - ETA: 8s - loss: 1.8912 - acc: 0.250 - ETA: 8s - loss: 1.8859 - acc: 0.268 - ETA: 8s - loss: 1.8991 - acc: 0.260 - ETA: 8s - loss: 1.9084 - acc: 0.256 - ETA: 8s - loss: 1.9117 - acc: 0.247 - ETA: 8s - loss: 1.9034 - acc: 0.252 - ETA: 8s - loss: 1.9033 - acc: 0.252 - ETA: 7s - loss: 1.9000 - acc: 0.256 - ETA: 7s - loss: 1.8965 - acc: 0.259 - ETA: 7s - loss: 1.8994 - acc: 0.260 - ETA: 7s - loss: 1.8955 - acc: 0.263 - ETA: 7s - loss: 1.8951 - acc: 0.263 - ETA: 7s - loss: 1.8954 - acc: 0.263 - ETA: 7s - loss: 1.8918 - acc: 0.266 - ETA: 6s - loss: 1.8916 - acc: 0.266 - ETA: 6s - loss: 1.8920 - acc: 0.265 - ETA: 6s - loss: 1.8898 - acc: 0.264 - ETA: 6s - loss: 1.8858 - acc: 0.265 - ETA: 6s - loss: 1.8857 - acc: 0.266 - ETA: 6s - loss: 1.8846 - acc: 0.266 - ETA: 6s - loss: 1.8840 - acc: 0.268 - ETA: 6s - loss: 1.8849 - acc: 0.266 - ETA: 5s - loss: 1.8853 - acc: 0.267 - ETA: 5s - loss: 1.8841 - acc: 0.267 - ETA: 5s - loss: 1.8831 - acc: 0.268 - ETA: 5s - loss: 1.8817 - acc: 0.270 - ETA: 5s - loss: 1.8806 - acc: 0.269 - ETA: 5s - loss: 1.8786 - acc: 0.271 - ETA: 5s - loss: 1.8751 - acc: 0.273 - ETA: 5s - loss: 1.8736 - acc: 0.273 - ETA: 4s - loss: 1.8729 - acc: 0.274 - ETA: 4s - loss: 1.8709 - acc: 0.275 - ETA: 4s - loss: 1.8674 - acc: 0.277 - ETA: 4s - loss: 1.8671 - acc: 0.276 - ETA: 4s - loss: 1.8681 - acc: 0.276 - ETA: 4s - loss: 1.8639 - acc: 0.280 - ETA: 4s - loss: 1.8615 - acc: 0.281 - ETA: 4s - loss: 1.8615 - acc: 0.280 - ETA: 3s - loss: 1.8615 - acc: 0.279 - ETA: 3s - loss: 1.8600 - acc: 0.280 - ETA: 3s - loss: 1.8586 - acc: 0.280 - ETA: 3s - loss: 1.8580 - acc: 0.280 - ETA: 3s - loss: 1.8565 - acc: 0.280 - ETA: 3s - loss: 1.8539 - acc: 0.281 - ETA: 3s - loss: 1.8532 - acc: 0.283 - ETA: 3s - loss: 1.8522 - acc: 0.283 - ETA: 2s - loss: 1.8532 - acc: 0.281 - ETA: 2s - loss: 1.8531 - acc: 0.281 - ETA: 2s - loss: 1.8520 - acc: 0.281 - ETA: 2s - loss: 1.8513 - acc: 0.280 - ETA: 2s - loss: 1.8502 - acc: 0.281 - ETA: 2s - loss: 1.8485 - acc: 0.282 - ETA: 2s - loss: 1.8483 - acc: 0.281 - ETA: 1s - loss: 1.8471 - acc: 0.282 - ETA: 1s - loss: 1.8458 - acc: 0.283 - ETA: 1s - loss: 1.8464 - acc: 0.282 - ETA: 1s - loss: 1.8461 - acc: 0.283 - ETA: 1s - loss: 1.8452 - acc: 0.283 - ETA: 1s - loss: 1.8457 - acc: 0.282 - ETA: 1s - loss: 1.8453 - acc: 0.283 - ETA: 1s - loss: 1.8450 - acc: 0.283 - ETA: 0s - loss: 1.8436 - acc: 0.283 - ETA: 0s - loss: 1.8429 - acc: 0.283 - ETA: 0s - loss: 1.8421 - acc: 0.284 - ETA: 0s - loss: 1.8420 - acc: 0.284 - ETA: 0s - loss: 1.8414 - acc: 0.285 - ETA: 0s - loss: 1.8420 - acc: 0.285 - ETA: 0s - loss: 1.8400 - acc: 0.285 - ETA: 0s - loss: 1.8404 - acc: 0.285 - 10s 680us/step - loss: 1.8401 - acc: 0.2854 - val_loss: 1.7868 - val_acc: 0.3271\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14341/14341 [==============================] - ETA: 9s - loss: 1.7615 - acc: 0.320 - ETA: 9s - loss: 1.7556 - acc: 0.347 - ETA: 9s - loss: 1.7669 - acc: 0.326 - ETA: 8s - loss: 1.7772 - acc: 0.331 - ETA: 8s - loss: 1.7870 - acc: 0.318 - ETA: 8s - loss: 1.7744 - acc: 0.320 - ETA: 8s - loss: 1.7723 - acc: 0.325 - ETA: 8s - loss: 1.7691 - acc: 0.324 - ETA: 8s - loss: 1.7758 - acc: 0.320 - ETA: 8s - loss: 1.7724 - acc: 0.323 - ETA: 7s - loss: 1.7717 - acc: 0.327 - ETA: 7s - loss: 1.7629 - acc: 0.334 - ETA: 7s - loss: 1.7564 - acc: 0.338 - ETA: 7s - loss: 1.7576 - acc: 0.335 - ETA: 7s - loss: 1.7535 - acc: 0.336 - ETA: 7s - loss: 1.7563 - acc: 0.332 - ETA: 7s - loss: 1.7571 - acc: 0.333 - ETA: 6s - loss: 1.7538 - acc: 0.335 - ETA: 6s - loss: 1.7539 - acc: 0.334 - ETA: 6s - loss: 1.7532 - acc: 0.334 - ETA: 6s - loss: 1.7549 - acc: 0.331 - ETA: 6s - loss: 1.7581 - acc: 0.329 - ETA: 6s - loss: 1.7593 - acc: 0.329 - ETA: 6s - loss: 1.7594 - acc: 0.331 - ETA: 5s - loss: 1.7563 - acc: 0.333 - ETA: 5s - loss: 1.7569 - acc: 0.334 - ETA: 5s - loss: 1.7541 - acc: 0.336 - ETA: 5s - loss: 1.7557 - acc: 0.335 - ETA: 5s - loss: 1.7555 - acc: 0.336 - ETA: 5s - loss: 1.7571 - acc: 0.333 - ETA: 5s - loss: 1.7574 - acc: 0.332 - ETA: 5s - loss: 1.7580 - acc: 0.332 - ETA: 4s - loss: 1.7579 - acc: 0.333 - ETA: 4s - loss: 1.7586 - acc: 0.333 - ETA: 4s - loss: 1.7552 - acc: 0.333 - ETA: 4s - loss: 1.7554 - acc: 0.333 - ETA: 4s - loss: 1.7557 - acc: 0.333 - ETA: 4s - loss: 1.7533 - acc: 0.333 - ETA: 4s - loss: 1.7548 - acc: 0.333 - ETA: 4s - loss: 1.7554 - acc: 0.334 - ETA: 3s - loss: 1.7533 - acc: 0.335 - ETA: 3s - loss: 1.7519 - acc: 0.335 - ETA: 3s - loss: 1.7526 - acc: 0.335 - ETA: 3s - loss: 1.7530 - acc: 0.336 - ETA: 3s - loss: 1.7530 - acc: 0.337 - ETA: 3s - loss: 1.7545 - acc: 0.336 - ETA: 3s - loss: 1.7550 - acc: 0.335 - ETA: 3s - loss: 1.7545 - acc: 0.335 - ETA: 2s - loss: 1.7547 - acc: 0.335 - ETA: 2s - loss: 1.7565 - acc: 0.335 - ETA: 2s - loss: 1.7555 - acc: 0.335 - ETA: 2s - loss: 1.7554 - acc: 0.335 - ETA: 2s - loss: 1.7550 - acc: 0.335 - ETA: 2s - loss: 1.7551 - acc: 0.335 - ETA: 2s - loss: 1.7549 - acc: 0.336 - ETA: 2s - loss: 1.7559 - acc: 0.336 - ETA: 1s - loss: 1.7547 - acc: 0.336 - ETA: 1s - loss: 1.7543 - acc: 0.335 - ETA: 1s - loss: 1.7540 - acc: 0.336 - ETA: 1s - loss: 1.7524 - acc: 0.336 - ETA: 1s - loss: 1.7519 - acc: 0.336 - ETA: 1s - loss: 1.7523 - acc: 0.336 - ETA: 1s - loss: 1.7503 - acc: 0.337 - ETA: 0s - loss: 1.7494 - acc: 0.338 - ETA: 0s - loss: 1.7491 - acc: 0.338 - ETA: 0s - loss: 1.7485 - acc: 0.338 - ETA: 0s - loss: 1.7472 - acc: 0.339 - ETA: 0s - loss: 1.7467 - acc: 0.339 - ETA: 0s - loss: 1.7467 - acc: 0.339 - ETA: 0s - loss: 1.7459 - acc: 0.340 - ETA: 0s - loss: 1.7448 - acc: 0.340 - 10s 688us/step - loss: 1.7457 - acc: 0.3403 - val_loss: 1.7001 - val_acc: 0.3620\n",
      "Epoch 4/100\n",
      "14341/14341 [==============================] - ETA: 8s - loss: 1.6916 - acc: 0.355 - ETA: 8s - loss: 1.6508 - acc: 0.380 - ETA: 8s - loss: 1.6666 - acc: 0.368 - ETA: 8s - loss: 1.6805 - acc: 0.368 - ETA: 8s - loss: 1.6842 - acc: 0.371 - ETA: 8s - loss: 1.6878 - acc: 0.364 - ETA: 8s - loss: 1.6831 - acc: 0.366 - ETA: 8s - loss: 1.6989 - acc: 0.356 - ETA: 8s - loss: 1.6922 - acc: 0.354 - ETA: 7s - loss: 1.6836 - acc: 0.361 - ETA: 7s - loss: 1.6839 - acc: 0.363 - ETA: 7s - loss: 1.6834 - acc: 0.362 - ETA: 7s - loss: 1.6901 - acc: 0.357 - ETA: 7s - loss: 1.6929 - acc: 0.358 - ETA: 7s - loss: 1.6934 - acc: 0.357 - ETA: 7s - loss: 1.7005 - acc: 0.356 - ETA: 6s - loss: 1.7036 - acc: 0.356 - ETA: 6s - loss: 1.7054 - acc: 0.354 - ETA: 6s - loss: 1.7030 - acc: 0.357 - ETA: 6s - loss: 1.7053 - acc: 0.357 - ETA: 6s - loss: 1.7002 - acc: 0.361 - ETA: 6s - loss: 1.7049 - acc: 0.358 - ETA: 6s - loss: 1.7048 - acc: 0.358 - ETA: 6s - loss: 1.7034 - acc: 0.358 - ETA: 5s - loss: 1.7020 - acc: 0.359 - ETA: 5s - loss: 1.7062 - acc: 0.357 - ETA: 5s - loss: 1.7063 - acc: 0.357 - ETA: 5s - loss: 1.7065 - acc: 0.357 - ETA: 5s - loss: 1.7084 - acc: 0.355 - ETA: 5s - loss: 1.7070 - acc: 0.355 - ETA: 5s - loss: 1.7064 - acc: 0.355 - ETA: 5s - loss: 1.7079 - acc: 0.354 - ETA: 4s - loss: 1.7098 - acc: 0.354 - ETA: 4s - loss: 1.7114 - acc: 0.352 - ETA: 4s - loss: 1.7136 - acc: 0.352 - ETA: 4s - loss: 1.7121 - acc: 0.354 - ETA: 4s - loss: 1.7147 - acc: 0.352 - ETA: 4s - loss: 1.7139 - acc: 0.353 - ETA: 4s - loss: 1.7146 - acc: 0.352 - ETA: 4s - loss: 1.7138 - acc: 0.353 - ETA: 3s - loss: 1.7140 - acc: 0.354 - ETA: 3s - loss: 1.7120 - acc: 0.354 - ETA: 3s - loss: 1.7119 - acc: 0.354 - ETA: 3s - loss: 1.7107 - acc: 0.354 - ETA: 3s - loss: 1.7104 - acc: 0.353 - ETA: 3s - loss: 1.7107 - acc: 0.352 - ETA: 3s - loss: 1.7117 - acc: 0.352 - ETA: 3s - loss: 1.7121 - acc: 0.353 - ETA: 2s - loss: 1.7125 - acc: 0.353 - ETA: 2s - loss: 1.7114 - acc: 0.354 - ETA: 2s - loss: 1.7109 - acc: 0.354 - ETA: 2s - loss: 1.7101 - acc: 0.355 - ETA: 2s - loss: 1.7115 - acc: 0.354 - ETA: 2s - loss: 1.7124 - acc: 0.354 - ETA: 2s - loss: 1.7129 - acc: 0.353 - ETA: 2s - loss: 1.7119 - acc: 0.354 - ETA: 1s - loss: 1.7117 - acc: 0.353 - ETA: 1s - loss: 1.7125 - acc: 0.353 - ETA: 1s - loss: 1.7108 - acc: 0.355 - ETA: 1s - loss: 1.7104 - acc: 0.355 - ETA: 1s - loss: 1.7077 - acc: 0.356 - ETA: 1s - loss: 1.7080 - acc: 0.357 - ETA: 1s - loss: 1.7058 - acc: 0.358 - ETA: 0s - loss: 1.7057 - acc: 0.358 - ETA: 0s - loss: 1.7048 - acc: 0.358 - ETA: 0s - loss: 1.7039 - acc: 0.358 - ETA: 0s - loss: 1.7041 - acc: 0.359 - ETA: 0s - loss: 1.7033 - acc: 0.360 - ETA: 0s - loss: 1.7029 - acc: 0.360 - ETA: 0s - loss: 1.7018 - acc: 0.361 - ETA: 0s - loss: 1.7024 - acc: 0.360 - 10s 688us/step - loss: 1.7020 - acc: 0.3605 - val_loss: 1.6389 - val_acc: 0.3851\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14341/14341 [==============================] - ETA: 8s - loss: 1.7017 - acc: 0.360 - ETA: 8s - loss: 1.7461 - acc: 0.330 - ETA: 8s - loss: 1.7116 - acc: 0.345 - ETA: 8s - loss: 1.6842 - acc: 0.363 - ETA: 8s - loss: 1.6758 - acc: 0.365 - ETA: 8s - loss: 1.6795 - acc: 0.365 - ETA: 8s - loss: 1.6750 - acc: 0.371 - ETA: 7s - loss: 1.6670 - acc: 0.374 - ETA: 7s - loss: 1.6664 - acc: 0.376 - ETA: 7s - loss: 1.6636 - acc: 0.380 - ETA: 7s - loss: 1.6704 - acc: 0.377 - ETA: 7s - loss: 1.6646 - acc: 0.380 - ETA: 7s - loss: 1.6587 - acc: 0.383 - ETA: 7s - loss: 1.6584 - acc: 0.385 - ETA: 7s - loss: 1.6561 - acc: 0.385 - ETA: 7s - loss: 1.6575 - acc: 0.382 - ETA: 6s - loss: 1.6589 - acc: 0.382 - ETA: 6s - loss: 1.6594 - acc: 0.381 - ETA: 6s - loss: 1.6650 - acc: 0.379 - ETA: 6s - loss: 1.6602 - acc: 0.382 - ETA: 6s - loss: 1.6543 - acc: 0.385 - ETA: 6s - loss: 1.6522 - acc: 0.386 - ETA: 6s - loss: 1.6513 - acc: 0.387 - ETA: 6s - loss: 1.6540 - acc: 0.386 - ETA: 5s - loss: 1.6533 - acc: 0.388 - ETA: 5s - loss: 1.6531 - acc: 0.388 - ETA: 5s - loss: 1.6558 - acc: 0.387 - ETA: 5s - loss: 1.6555 - acc: 0.388 - ETA: 5s - loss: 1.6574 - acc: 0.387 - ETA: 5s - loss: 1.6590 - acc: 0.387 - ETA: 5s - loss: 1.6573 - acc: 0.387 - ETA: 5s - loss: 1.6566 - acc: 0.388 - ETA: 4s - loss: 1.6570 - acc: 0.388 - ETA: 4s - loss: 1.6576 - acc: 0.387 - ETA: 4s - loss: 1.6568 - acc: 0.387 - ETA: 4s - loss: 1.6556 - acc: 0.387 - ETA: 4s - loss: 1.6554 - acc: 0.387 - ETA: 4s - loss: 1.6555 - acc: 0.387 - ETA: 4s - loss: 1.6522 - acc: 0.387 - ETA: 4s - loss: 1.6533 - acc: 0.386 - ETA: 3s - loss: 1.6511 - acc: 0.387 - ETA: 3s - loss: 1.6514 - acc: 0.388 - ETA: 3s - loss: 1.6488 - acc: 0.389 - ETA: 3s - loss: 1.6484 - acc: 0.389 - ETA: 3s - loss: 1.6498 - acc: 0.389 - ETA: 3s - loss: 1.6513 - acc: 0.388 - ETA: 3s - loss: 1.6525 - acc: 0.388 - ETA: 3s - loss: 1.6546 - acc: 0.387 - ETA: 2s - loss: 1.6538 - acc: 0.388 - ETA: 2s - loss: 1.6535 - acc: 0.388 - ETA: 2s - loss: 1.6528 - acc: 0.388 - ETA: 2s - loss: 1.6533 - acc: 0.388 - ETA: 2s - loss: 1.6529 - acc: 0.387 - ETA: 2s - loss: 1.6525 - acc: 0.387 - ETA: 2s - loss: 1.6524 - acc: 0.387 - ETA: 1s - loss: 1.6504 - acc: 0.387 - ETA: 1s - loss: 1.6507 - acc: 0.387 - ETA: 1s - loss: 1.6523 - acc: 0.386 - ETA: 1s - loss: 1.6524 - acc: 0.387 - ETA: 1s - loss: 1.6524 - acc: 0.386 - ETA: 1s - loss: 1.6515 - acc: 0.387 - ETA: 1s - loss: 1.6500 - acc: 0.387 - ETA: 1s - loss: 1.6495 - acc: 0.387 - ETA: 0s - loss: 1.6493 - acc: 0.387 - ETA: 0s - loss: 1.6468 - acc: 0.389 - ETA: 0s - loss: 1.6459 - acc: 0.389 - ETA: 0s - loss: 1.6466 - acc: 0.389 - ETA: 0s - loss: 1.6471 - acc: 0.389 - ETA: 0s - loss: 1.6482 - acc: 0.387 - ETA: 0s - loss: 1.6497 - acc: 0.386 - ETA: 0s - loss: 1.6497 - acc: 0.386 - 10s 684us/step - loss: 1.6485 - acc: 0.3873 - val_loss: 1.6214 - val_acc: 0.4018\n",
      "Epoch 6/100\n",
      "14341/14341 [==============================] - ETA: 8s - loss: 1.7043 - acc: 0.365 - ETA: 8s - loss: 1.6339 - acc: 0.380 - ETA: 8s - loss: 1.6165 - acc: 0.393 - ETA: 8s - loss: 1.6429 - acc: 0.385 - ETA: 8s - loss: 1.6483 - acc: 0.383 - ETA: 8s - loss: 1.6544 - acc: 0.379 - ETA: 8s - loss: 1.6603 - acc: 0.376 - ETA: 7s - loss: 1.6546 - acc: 0.379 - ETA: 7s - loss: 1.6560 - acc: 0.379 - ETA: 7s - loss: 1.6663 - acc: 0.372 - ETA: 7s - loss: 1.6665 - acc: 0.373 - ETA: 7s - loss: 1.6583 - acc: 0.375 - ETA: 7s - loss: 1.6538 - acc: 0.380 - ETA: 7s - loss: 1.6529 - acc: 0.380 - ETA: 7s - loss: 1.6542 - acc: 0.380 - ETA: 7s - loss: 1.6527 - acc: 0.381 - ETA: 6s - loss: 1.6471 - acc: 0.383 - ETA: 6s - loss: 1.6514 - acc: 0.380 - ETA: 6s - loss: 1.6497 - acc: 0.380 - ETA: 6s - loss: 1.6510 - acc: 0.380 - ETA: 6s - loss: 1.6472 - acc: 0.381 - ETA: 6s - loss: 1.6414 - acc: 0.384 - ETA: 6s - loss: 1.6390 - acc: 0.385 - ETA: 6s - loss: 1.6365 - acc: 0.386 - ETA: 5s - loss: 1.6347 - acc: 0.386 - ETA: 5s - loss: 1.6301 - acc: 0.389 - ETA: 5s - loss: 1.6315 - acc: 0.388 - ETA: 5s - loss: 1.6314 - acc: 0.389 - ETA: 5s - loss: 1.6325 - acc: 0.388 - ETA: 5s - loss: 1.6328 - acc: 0.388 - ETA: 5s - loss: 1.6332 - acc: 0.389 - ETA: 5s - loss: 1.6327 - acc: 0.388 - ETA: 4s - loss: 1.6323 - acc: 0.389 - ETA: 4s - loss: 1.6307 - acc: 0.390 - ETA: 4s - loss: 1.6304 - acc: 0.388 - ETA: 4s - loss: 1.6334 - acc: 0.387 - ETA: 4s - loss: 1.6293 - acc: 0.389 - ETA: 4s - loss: 1.6286 - acc: 0.389 - ETA: 4s - loss: 1.6248 - acc: 0.391 - ETA: 4s - loss: 1.6222 - acc: 0.393 - ETA: 3s - loss: 1.6212 - acc: 0.394 - ETA: 3s - loss: 1.6188 - acc: 0.395 - ETA: 3s - loss: 1.6213 - acc: 0.395 - ETA: 3s - loss: 1.6208 - acc: 0.395 - ETA: 3s - loss: 1.6218 - acc: 0.394 - ETA: 3s - loss: 1.6204 - acc: 0.395 - ETA: 3s - loss: 1.6197 - acc: 0.395 - ETA: 3s - loss: 1.6194 - acc: 0.396 - ETA: 2s - loss: 1.6199 - acc: 0.395 - ETA: 2s - loss: 1.6193 - acc: 0.396 - ETA: 2s - loss: 1.6226 - acc: 0.394 - ETA: 2s - loss: 1.6239 - acc: 0.393 - ETA: 2s - loss: 1.6258 - acc: 0.392 - ETA: 2s - loss: 1.6239 - acc: 0.393 - ETA: 2s - loss: 1.6240 - acc: 0.393 - ETA: 2s - loss: 1.6256 - acc: 0.392 - ETA: 1s - loss: 1.6264 - acc: 0.392 - ETA: 1s - loss: 1.6276 - acc: 0.391 - ETA: 1s - loss: 1.6283 - acc: 0.391 - ETA: 1s - loss: 1.6302 - acc: 0.390 - ETA: 1s - loss: 1.6298 - acc: 0.390 - ETA: 1s - loss: 1.6291 - acc: 0.391 - ETA: 1s - loss: 1.6295 - acc: 0.391 - ETA: 0s - loss: 1.6288 - acc: 0.391 - ETA: 0s - loss: 1.6276 - acc: 0.392 - ETA: 0s - loss: 1.6264 - acc: 0.392 - ETA: 0s - loss: 1.6268 - acc: 0.393 - ETA: 0s - loss: 1.6254 - acc: 0.393 - ETA: 0s - loss: 1.6264 - acc: 0.393 - ETA: 0s - loss: 1.6256 - acc: 0.393 - ETA: 0s - loss: 1.6255 - acc: 0.393 - 10s 688us/step - loss: 1.6253 - acc: 0.3931 - val_loss: 1.5977 - val_acc: 0.4066\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14341/14341 [==============================] - ETA: 8s - loss: 1.6738 - acc: 0.335 - ETA: 8s - loss: 1.6098 - acc: 0.390 - ETA: 8s - loss: 1.5988 - acc: 0.408 - ETA: 8s - loss: 1.5785 - acc: 0.415 - ETA: 8s - loss: 1.5835 - acc: 0.420 - ETA: 8s - loss: 1.5853 - acc: 0.419 - ETA: 8s - loss: 1.5846 - acc: 0.422 - ETA: 8s - loss: 1.5951 - acc: 0.415 - ETA: 7s - loss: 1.5981 - acc: 0.412 - ETA: 7s - loss: 1.6038 - acc: 0.410 - ETA: 7s - loss: 1.5993 - acc: 0.416 - ETA: 7s - loss: 1.5998 - acc: 0.416 - ETA: 7s - loss: 1.5967 - acc: 0.416 - ETA: 7s - loss: 1.5912 - acc: 0.419 - ETA: 7s - loss: 1.5959 - acc: 0.417 - ETA: 7s - loss: 1.5973 - acc: 0.417 - ETA: 6s - loss: 1.5948 - acc: 0.415 - ETA: 6s - loss: 1.6018 - acc: 0.413 - ETA: 6s - loss: 1.6014 - acc: 0.412 - ETA: 6s - loss: 1.5936 - acc: 0.416 - ETA: 6s - loss: 1.5945 - acc: 0.414 - ETA: 6s - loss: 1.5936 - acc: 0.415 - ETA: 6s - loss: 1.5948 - acc: 0.413 - ETA: 6s - loss: 1.5934 - acc: 0.413 - ETA: 5s - loss: 1.5950 - acc: 0.411 - ETA: 5s - loss: 1.5922 - acc: 0.413 - ETA: 5s - loss: 1.5931 - acc: 0.412 - ETA: 5s - loss: 1.5921 - acc: 0.411 - ETA: 5s - loss: 1.5948 - acc: 0.410 - ETA: 5s - loss: 1.5934 - acc: 0.410 - ETA: 5s - loss: 1.5942 - acc: 0.410 - ETA: 5s - loss: 1.5919 - acc: 0.410 - ETA: 4s - loss: 1.5926 - acc: 0.410 - ETA: 4s - loss: 1.5942 - acc: 0.409 - ETA: 4s - loss: 1.5901 - acc: 0.411 - ETA: 4s - loss: 1.5905 - acc: 0.411 - ETA: 4s - loss: 1.5886 - acc: 0.411 - ETA: 4s - loss: 1.5882 - acc: 0.411 - ETA: 4s - loss: 1.5878 - acc: 0.411 - ETA: 4s - loss: 1.5864 - acc: 0.412 - ETA: 3s - loss: 1.5855 - acc: 0.412 - ETA: 3s - loss: 1.5852 - acc: 0.412 - ETA: 3s - loss: 1.5850 - acc: 0.412 - ETA: 3s - loss: 1.5845 - acc: 0.412 - ETA: 3s - loss: 1.5828 - acc: 0.413 - ETA: 3s - loss: 1.5827 - acc: 0.412 - ETA: 3s - loss: 1.5828 - acc: 0.413 - ETA: 3s - loss: 1.5805 - acc: 0.414 - ETA: 2s - loss: 1.5806 - acc: 0.414 - ETA: 2s - loss: 1.5799 - acc: 0.414 - ETA: 2s - loss: 1.5790 - acc: 0.415 - ETA: 2s - loss: 1.5802 - acc: 0.414 - ETA: 2s - loss: 1.5794 - acc: 0.414 - ETA: 2s - loss: 1.5779 - acc: 0.414 - ETA: 2s - loss: 1.5796 - acc: 0.412 - ETA: 2s - loss: 1.5804 - acc: 0.412 - ETA: 1s - loss: 1.5810 - acc: 0.412 - ETA: 1s - loss: 1.5810 - acc: 0.412 - ETA: 1s - loss: 1.5814 - acc: 0.413 - ETA: 1s - loss: 1.5810 - acc: 0.413 - ETA: 1s - loss: 1.5804 - acc: 0.412 - ETA: 1s - loss: 1.5787 - acc: 0.413 - ETA: 1s - loss: 1.5771 - acc: 0.415 - ETA: 0s - loss: 1.5780 - acc: 0.415 - ETA: 0s - loss: 1.5779 - acc: 0.416 - ETA: 0s - loss: 1.5791 - acc: 0.415 - ETA: 0s - loss: 1.5792 - acc: 0.415 - ETA: 0s - loss: 1.5767 - acc: 0.415 - ETA: 0s - loss: 1.5776 - acc: 0.415 - ETA: 0s - loss: 1.5795 - acc: 0.414 - ETA: 0s - loss: 1.5794 - acc: 0.413 - 10s 687us/step - loss: 1.5788 - acc: 0.4142 - val_loss: 1.5496 - val_acc: 0.4339\n",
      "Epoch 8/100\n",
      "14341/14341 [==============================] - ETA: 9s - loss: 1.5548 - acc: 0.460 - ETA: 8s - loss: 1.5843 - acc: 0.420 - ETA: 9s - loss: 1.5703 - acc: 0.418 - ETA: 8s - loss: 1.5453 - acc: 0.422 - ETA: 8s - loss: 1.5384 - acc: 0.425 - ETA: 8s - loss: 1.5396 - acc: 0.427 - ETA: 8s - loss: 1.5347 - acc: 0.429 - ETA: 8s - loss: 1.5317 - acc: 0.434 - ETA: 8s - loss: 1.5365 - acc: 0.432 - ETA: 7s - loss: 1.5389 - acc: 0.432 - ETA: 7s - loss: 1.5340 - acc: 0.433 - ETA: 7s - loss: 1.5415 - acc: 0.427 - ETA: 7s - loss: 1.5480 - acc: 0.425 - ETA: 7s - loss: 1.5466 - acc: 0.424 - ETA: 7s - loss: 1.5426 - acc: 0.424 - ETA: 7s - loss: 1.5467 - acc: 0.421 - ETA: 6s - loss: 1.5521 - acc: 0.420 - ETA: 6s - loss: 1.5567 - acc: 0.418 - ETA: 6s - loss: 1.5553 - acc: 0.420 - ETA: 6s - loss: 1.5546 - acc: 0.420 - ETA: 6s - loss: 1.5509 - acc: 0.420 - ETA: 6s - loss: 1.5546 - acc: 0.419 - ETA: 6s - loss: 1.5549 - acc: 0.419 - ETA: 6s - loss: 1.5546 - acc: 0.418 - ETA: 5s - loss: 1.5568 - acc: 0.417 - ETA: 5s - loss: 1.5547 - acc: 0.418 - ETA: 5s - loss: 1.5533 - acc: 0.419 - ETA: 5s - loss: 1.5530 - acc: 0.419 - ETA: 5s - loss: 1.5543 - acc: 0.418 - ETA: 5s - loss: 1.5545 - acc: 0.416 - ETA: 5s - loss: 1.5541 - acc: 0.417 - ETA: 5s - loss: 1.5572 - acc: 0.415 - ETA: 4s - loss: 1.5578 - acc: 0.415 - ETA: 4s - loss: 1.5581 - acc: 0.415 - ETA: 4s - loss: 1.5582 - acc: 0.416 - ETA: 4s - loss: 1.5596 - acc: 0.416 - ETA: 4s - loss: 1.5572 - acc: 0.417 - ETA: 4s - loss: 1.5583 - acc: 0.417 - ETA: 4s - loss: 1.5572 - acc: 0.418 - ETA: 4s - loss: 1.5564 - acc: 0.418 - ETA: 3s - loss: 1.5560 - acc: 0.419 - ETA: 3s - loss: 1.5529 - acc: 0.421 - ETA: 3s - loss: 1.5511 - acc: 0.422 - ETA: 3s - loss: 1.5495 - acc: 0.423 - ETA: 3s - loss: 1.5485 - acc: 0.424 - ETA: 3s - loss: 1.5480 - acc: 0.425 - ETA: 3s - loss: 1.5484 - acc: 0.425 - ETA: 3s - loss: 1.5478 - acc: 0.424 - ETA: 2s - loss: 1.5501 - acc: 0.423 - ETA: 2s - loss: 1.5486 - acc: 0.423 - ETA: 2s - loss: 1.5482 - acc: 0.422 - ETA: 2s - loss: 1.5467 - acc: 0.423 - ETA: 2s - loss: 1.5488 - acc: 0.422 - ETA: 2s - loss: 1.5499 - acc: 0.422 - ETA: 2s - loss: 1.5501 - acc: 0.422 - ETA: 1s - loss: 1.5509 - acc: 0.422 - ETA: 1s - loss: 1.5490 - acc: 0.423 - ETA: 1s - loss: 1.5469 - acc: 0.424 - ETA: 1s - loss: 1.5460 - acc: 0.424 - ETA: 1s - loss: 1.5469 - acc: 0.423 - ETA: 1s - loss: 1.5476 - acc: 0.423 - ETA: 1s - loss: 1.5468 - acc: 0.423 - ETA: 1s - loss: 1.5470 - acc: 0.424 - ETA: 0s - loss: 1.5467 - acc: 0.424 - ETA: 0s - loss: 1.5463 - acc: 0.424 - ETA: 0s - loss: 1.5471 - acc: 0.424 - ETA: 0s - loss: 1.5473 - acc: 0.425 - ETA: 0s - loss: 1.5464 - acc: 0.425 - ETA: 0s - loss: 1.5453 - acc: 0.425 - ETA: 0s - loss: 1.5469 - acc: 0.424 - ETA: 0s - loss: 1.5492 - acc: 0.424 - 10s 684us/step - loss: 1.5494 - acc: 0.4245 - val_loss: 1.5545 - val_acc: 0.4222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6f7d7be0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cnn모델 학습\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping()\n",
    "keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16, 3, 3, border_mode='same', activation='relu', #3x3필터 16개를 적용, same:입력이미지와 출력 이미지크기같음\n",
    "                        input_shape=data_train_image.shape[1:]))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #2,2 중 가장 높은값을 가져옴 - 사소한 특징 무시\n",
    "model.add(Dropout(0.25))\n",
    "  \n",
    "model.add(Convolution2D(64, 3, 3,  activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "  \n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8,activation = 'softmax'))\n",
    "  \n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "model.fit(data_train_image, labels_train_image, batch_size=200,validation_split = 0.2, nb_epoch=100,callbacks=[early_stopping])\n",
    "\n",
    "# model.save('Gersang.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#이미지 샘플링 할 경우\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping()\n",
    "# keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# model = Sequential()\n",
    "# model.add(Convolution2D(16, 3, 3, border_mode='same', activation='relu', #3x3필터 16개를 적용, same:입력이미지와 출력 이미지크기같음\n",
    "#                         input_shape=data_train_image.shape[1:]))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2))) #2,2 중 가장 높은값을 가져옴 - 사소한 특징 무시\n",
    "# model.add(Dropout(0.25))\n",
    "  \n",
    "# model.add(Convolution2D(64, 3, 3,  activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    " \n",
    "# model.add(Convolution2D(64, 3, 3))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "  \n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation = 'relu')) # 256\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(8,activation = 'softmax'))\n",
    "  \n",
    "# model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "# #이미지 겹치기로 늘려주면 좋음\n",
    "# datagen = ImageDataGenerator(\n",
    "#         rotation_range=40,\n",
    "#         width_shift_range=0.2,\n",
    "#         height_shift_range=0.2,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True,\n",
    "#         fill_mode='nearest') #try constant\n",
    "\n",
    "# datagen.fit(data_train_image)\n",
    "\n",
    "# model.fit_generator(datagen.flow(data_train_image,labels_train_image,batch_size =3),validation_data=(data_test_image, labels_test_image),verbose = 2, epochs=100,steps_per_epoch = len(data_test_image) / 32,callbacks=[early_stopping])\n",
    "\n",
    "# model.save('Gersang.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5976/5976 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 245us/step\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(data_test_image,labels_test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 1.685\n",
      "  Accuracy: 0.368\n"
     ]
    }
   ],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "320/320 [==============================] - ETA: 15:00 - loss: nan - acc: 0.333 - ETA: 2:08 - loss: nan - acc: 0.190 - ETA: 1:09 - loss: nan - acc: 0.17 - ETA: 44s - loss: nan - acc: 0.1500 - ETA: 33s - loss: nan - acc: 0.148 - ETA: 26s - loss: nan - acc: 0.166 - ETA: 21s - loss: nan - acc: 0.178 - ETA: 18s - loss: nan - acc: 0.170 - ETA: 16s - loss: nan - acc: 0.166 - ETA: 14s - loss: nan - acc: 0.163 - ETA: 12s - loss: nan - acc: 0.171 - ETA: 11s - loss: nan - acc: 0.164 - ETA: 10s - loss: nan - acc: 0.158 - ETA: 9s - loss: nan - acc: 0.161 - ETA: 8s - loss: nan - acc: 0.15 - ETA: 7s - loss: nan - acc: 0.16 - ETA: 7s - loss: nan - acc: 0.16 - ETA: 6s - loss: nan - acc: 0.16 - ETA: 6s - loss: nan - acc: 0.16 - ETA: 5s - loss: nan - acc: 0.16 - ETA: 5s - loss: nan - acc: 0.16 - ETA: 4s - loss: nan - acc: 0.16 - ETA: 4s - loss: nan - acc: 0.16 - ETA: 4s - loss: nan - acc: 0.16 - ETA: 3s - loss: nan - acc: 0.16 - ETA: 3s - loss: nan - acc: 0.16 - ETA: 3s - loss: nan - acc: 0.16 - ETA: 3s - loss: nan - acc: 0.16 - ETA: 2s - loss: nan - acc: 0.16 - ETA: 2s - loss: nan - acc: 0.16 - ETA: 2s - loss: nan - acc: 0.15 - ETA: 2s - loss: nan - acc: 0.15 - ETA: 2s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - 17s 52ms/step - loss: nan - acc: 0.1656 - val_loss: nan - val_acc: 0.1605\n",
      "Epoch 2/5\n",
      "320/320 [==============================] - ETA: 4s - loss: nan - acc: 0.0000e+ - ETA: 3s - loss: nan - acc: 0.0556   - ETA: 3s - loss: nan - acc: 0.19 - ETA: 3s - loss: nan - acc: 0.18 - ETA: 2s - loss: nan - acc: 0.20 - ETA: 2s - loss: nan - acc: 0.18 - ETA: 2s - loss: nan - acc: 0.19 - ETA: 2s - loss: nan - acc: 0.18 - ETA: 2s - loss: nan - acc: 0.19 - ETA: 2s - loss: nan - acc: 0.18 - ETA: 2s - loss: nan - acc: 0.16 - ETA: 2s - loss: nan - acc: 0.15 - ETA: 2s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.15 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.15 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.15 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.15 - ETA: 0s - loss: nan - acc: 0.15 - 13s 40ms/step - loss: nan - acc: 0.1583 - val_loss: nan - val_acc: 0.1605\n",
      "Epoch 3/5\n",
      "320/320 [==============================] - ETA: 3s - loss: nan - acc: 0.0000e+ - ETA: 2s - loss: nan - acc: 0.0952   - ETA: 2s - loss: nan - acc: 0.09 - ETA: 2s - loss: nan - acc: 0.07 - ETA: 2s - loss: nan - acc: 0.08 - ETA: 2s - loss: nan - acc: 0.08 - ETA: 2s - loss: nan - acc: 0.11 - ETA: 2s - loss: nan - acc: 0.10 - ETA: 2s - loss: nan - acc: 0.11 - ETA: 2s - loss: nan - acc: 0.10 - ETA: 2s - loss: nan - acc: 0.11 - ETA: 2s - loss: nan - acc: 0.12 - ETA: 2s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.15 - ETA: 1s - loss: nan - acc: 0.15 - ETA: 1s - loss: nan - acc: 0.15 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 0s - loss: nan - acc: 0.14 - ETA: 0s - loss: nan - acc: 0.15 - ETA: 0s - loss: nan - acc: 0.15 - ETA: 0s - loss: nan - acc: 0.15 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.17 - 13s 40ms/step - loss: nan - acc: 0.1698 - val_loss: nan - val_acc: 0.1605\n",
      "Epoch 4/5\n",
      "320/320 [==============================] - ETA: 3s - loss: nan - acc: 0.66 - ETA: 3s - loss: nan - acc: 0.26 - ETA: 3s - loss: nan - acc: 0.18 - ETA: 2s - loss: nan - acc: 0.17 - ETA: 2s - loss: nan - acc: 0.17 - ETA: 2s - loss: nan - acc: 0.19 - ETA: 2s - loss: nan - acc: 0.16 - ETA: 2s - loss: nan - acc: 0.16 - ETA: 2s - loss: nan - acc: 0.15 - ETA: 2s - loss: nan - acc: 0.14 - ETA: 2s - loss: nan - acc: 0.13 - ETA: 2s - loss: nan - acc: 0.13 - ETA: 2s - loss: nan - acc: 0.13 - ETA: 2s - loss: nan - acc: 0.14 - ETA: 2s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.12 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.12 - ETA: 1s - loss: nan - acc: 0.12 - ETA: 1s - loss: nan - acc: 0.12 - ETA: 1s - loss: nan - acc: 0.12 - ETA: 1s - loss: nan - acc: 0.12 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.13 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 1s - loss: nan - acc: 0.14 - ETA: 0s - loss: nan - acc: 0.14 - ETA: 0s - loss: nan - acc: 0.14 - ETA: 0s - loss: nan - acc: 0.13 - ETA: 0s - loss: nan - acc: 0.14 - ETA: 0s - loss: nan - acc: 0.13 - ETA: 0s - loss: nan - acc: 0.13 - ETA: 0s - loss: nan - acc: 0.14 - ETA: 0s - loss: nan - acc: 0.14 - ETA: 0s - loss: nan - acc: 0.13 - ETA: 0s - loss: nan - acc: 0.13 - ETA: 0s - loss: nan - acc: 0.13 - ETA: 0s - loss: nan - acc: 0.13 - ETA: 0s - loss: nan - acc: 0.13 - ETA: 0s - loss: nan - acc: 0.13 - ETA: 0s - loss: nan - acc: 0.14 - ETA: 0s - loss: nan - acc: 0.14 - ETA: 0s - loss: nan - acc: 0.14 - 13s 41ms/step - loss: nan - acc: 0.1448 - val_loss: nan - val_acc: 0.1605\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - ETA: 3s - loss: nan - acc: 0.0000e+ - ETA: 3s - loss: nan - acc: 0.2222   - ETA: 3s - loss: nan - acc: 0.27 - ETA: 2s - loss: nan - acc: 0.19 - ETA: 2s - loss: nan - acc: 0.17 - ETA: 2s - loss: nan - acc: 0.19 - ETA: 2s - loss: nan - acc: 0.19 - ETA: 2s - loss: nan - acc: 0.21 - ETA: 2s - loss: nan - acc: 0.21 - ETA: 2s - loss: nan - acc: 0.20 - ETA: 2s - loss: nan - acc: 0.19 - ETA: 2s - loss: nan - acc: 0.19 - ETA: 2s - loss: nan - acc: 0.18 - ETA: 1s - loss: nan - acc: 0.18 - ETA: 1s - loss: nan - acc: 0.18 - ETA: 1s - loss: nan - acc: 0.18 - ETA: 1s - loss: nan - acc: 0.18 - ETA: 1s - loss: nan - acc: 0.18 - ETA: 1s - loss: nan - acc: 0.18 - ETA: 1s - loss: nan - acc: 0.17 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 1s - loss: nan - acc: 0.17 - ETA: 1s - loss: nan - acc: 0.17 - ETA: 1s - loss: nan - acc: 0.17 - ETA: 1s - loss: nan - acc: 0.18 - ETA: 1s - loss: nan - acc: 0.18 - ETA: 1s - loss: nan - acc: 0.17 - ETA: 1s - loss: nan - acc: 0.17 - ETA: 1s - loss: nan - acc: 0.17 - ETA: 1s - loss: nan - acc: 0.17 - ETA: 1s - loss: nan - acc: 0.17 - ETA: 1s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.17 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - ETA: 0s - loss: nan - acc: 0.16 - 13s 40ms/step - loss: nan - acc: 0.1677 - val_loss: nan - val_acc: 0.1605\n"
     ]
    }
   ],
   "source": [
    "# inception v3 trans learning\n",
    "import tensorflow as tf\n",
    "import keras.applications\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "CLASSES = 8\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "predictions = Dense(CLASSES, activation='softmax')(x)\n",
    "prev_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "model = Sequential()\n",
    "model.add(prev_model)\n",
    "\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "\n",
    "model.add(Dense(8,activation = 'softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "datagen.fit(data_train_image)\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = 320\n",
    "VALIDATION_STEPS = 64\n",
    "MODEL_FILE = 'filename.model'\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(data_train_image,labels_train_image,batch_size =3),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=(data_test_image, labels_test_image),\n",
    "    validation_steps=VALIDATION_STEPS)\n",
    "  \n",
    "\n",
    "# model.save(MODEL_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
